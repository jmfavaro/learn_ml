---
title: "ISLR Notebook"
output:
  html_notebook: 
    toc: yes
---

Notebook pour le livre "Introduction to Statistical Learning with Applications in R" 
(url)[http://www-bcf.usc.edu/~gareth/ISL/]

MOOC : https://lagunita.stanford.edu/dashboard

id : jmfavaro@yahoo.fr  
pwd : JM70vinn

videos : [https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/]



```{r message=FALSE}
library(ISLR)
```
```{python}
np = [1,2,3]
print(np)
print(type(np))
```

# Chapitre 2 : Statistical Learning

- prédiction **vs** l'inférence

- methode paramétrique **vs** non-paramétrique : la forme de **f** est défini vs la forme ne l'est pas.
les méthodes non paramétriques sont généralement utilisés pour la prédiction car meilleure pour ça. cela dit, pas toujours car ces méthodes sont sensibles à l'*overfitting*.

- supervised **vs** unsupervised 

- regression **vs** classification pb : selon le type de variable à modéliser (quant ou quali). 

## Assessing model accuracy

### regression settings

pour la regression : MSE. calcul pour le training dataset et le test dataset.
overfitting : le training est très faible mais le test est fort
=> cross validation

- the bias-variance trade off

expected test MSE se décompose en 3 parties:
    - variance
    - squared biais
    - variance de l'erreur
    
la variance : variation du à l'échantillon (on change d'échantillon, on a des coef un peu différents). plus la methode stat est flexible, plus la variance est forte en général.
biais : provient des simplifications faites sur la forme fonctionnelle (un mod lin peut avoir un très fort biais si la relation n'est pas linéaire). plus un mothode est flexible, moins le biais est fort (en général).
donc plus la methode est flexible, plus la variance augmente et plus le biais baisse. la point critique est la dynamique des 2.

ce trade-off est la principale difficulté. il ne faut jamais le perdre de vue. Il se manifeste par la courbe en U du taux d'erreur sur le test dataset.




### classification settings

- error rate

#### the bayes classifier:  affecter à la classe qui a la plus forte probabilité

- Bayes decision boundary : limite (frontiere) séparant les classes via le bayes classifier

- le bayes classifier produit le plus petit taux d'erreur (error rate), appelé le **bayes error rate**

#### K-Nearest Neighbors (KNN)

En theorie, on voudrait toujours utiliser le bayes classifier, mais en pratique, on on ne connait pas la distribution conditionnelle de Y sachant X. donc on peut pas le calculer (*?? pourquoi*)

Souvent très proche du bayes classifier. le choix de K joue énormément. Si K=1, modèle très flexible (overfitting), plus K grandit, moins le modèle est flexible et la frontière devient linéaire. Il n'y a pas de relation forte entre le taux d'erreur sur le training et le test dataset.


# Chapitre 3 : Linear Regression

## Some importants questions

### 1: relation entre la reponse et les predicteurs?

methode : 

- F-statistic teste b1=b2=...=bn=0 vs au moins un b est <> 0. ce critère est important car la p-value de chaque coefficient ne suffit pas. il y a de forte chance qu'il y est au moins un de ces coeffcicient qui ait par chance au moins un p-value < 0.005, c'est notamment le cas lorsqu'il y a bcp de prédicteurs.

- plusieurs methodes (indcicateurs) pour faire la selection de variable : Mallow's CP, AIC, BIC, adj-R² (voir chap 6)    

### 2: les methodes de selection (basé sur la p-value): 

    - **forward selection**
    - **backward selection**
    - **mixed selection** (stepwise?)
    

### 3: Model Fit

### 4: Predictions

### 5 : les problèmes

la colinéarité augmente la standard error des coefficients. donc fait baisser la la t-stat et donc on augmente la chance de rejecter H0 : b~j~ = 0. baisse de la **puissance** du test. la matrice de correlation ne permet que de voir les corrélation 2 à 2. En cas de multicolinéarité, on voit rien. Il faut utiliser le **VIF**. la valuer la plus faible est 1. Typiquement, une valeur qui dépasse 5 ou 10. Le VIF peut se calculer pour chaque vairable. Si la corrélation de X~i~  regressé sur toutes les autres est proche de 1 ,aalors il y a probabelement corrélation.

Que faire dans ce cas là : supprimer la variable. ou faire une combinaison des variables incriminées.

## LAB

### regression simple

```{r echo=TRUE, message=FALSE}
library(MASS)
library(ISLR)
names(Boston)

lm.fit <- lm(medv∼lstat ,data=Boston )

summary(lm.fit)

names(lm.fit)

confint(lm.fit)

```

### regression multiple

```{r echo=TRUE}
lm.fit =lm(medv∼lstat+age ,data=Boston )
summary (lm.fit)
```

#### calcul du VIF (avec la librairie CAR)

```{r echo=TRUE, message=FALSE}
library(car)

vif(lm.fit)

```

le VIF est inférieur à 5. c'est ok.

#### interaction entre variable:  var1:vaz2  mais ne prend que l'interactions

```{r echo=TRUE, message=FALSE}

lm.fit <- lm(medv∼lstat *age ,data=Boston )

summary(lm.fit)


```


##### transformations non linéaires

il faut utilise la fonction I() dans la lm. ci dessous au carré

```{r echo=TRUE, message=FALSE}

lm.fit2=lm(medv~lstat +I(lstat ^2), data = Boston)
summary (lm.fit2)

```

la fonction **anova()** permet de tester l'amélioration du modèle. on rentre les 2 modèles(par emboitement). bien sur il faut que les toues les variables du premier modèle soint dans le second. hypothese null : les 2 modèles ont le meme fit, hypotheses alternative le 2nd modèle est meilleur.

```{r echo=TRUE}
lm.fit =lm(medv∼lstat, data = Boston)
anova(lm.fit,lm.fit2)

```


si on beut plus de polynomes, on peut utiliser la fonction **poly(var, N)**

```{r echo = TRUE}

lm.fit5=lm(medv~poly(lstat ,5), data = Boston)
summary(lm.fit5)
```

#### les variables qualitatives

la fonction **contrast()** permet de connaitre (et modifier) le codage













